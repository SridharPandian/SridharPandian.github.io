<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-09-29T14:43:53-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">SridharPandian.github.io</title><subtitle>My version of github's personal-website repository.</subtitle><entry><title type="html">Dexterous Imitation Made Easy - A Learning-Based Framework for Efficient Dexterous Manipulation</title><link href="http://localhost:4000/2022/09/28/dime/" rel="alternate" type="text/html" title="Dexterous Imitation Made Easy - A Learning-Based Framework for Efficient Dexterous Manipulation" /><published>2022-09-28T19:39:20-04:00</published><updated>2022-09-28T19:39:20-04:00</updated><id>http://localhost:4000/2022/09/28/dime</id><content type="html" xml:base="http://localhost:4000/2022/09/28/dime/">&lt;p&gt;Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Perhaps one of the most powerful techniques to learn complex manipulation strategies is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging. The complex, high-dimensional action-space involved with multi-finger control often leads to poor sample efficiency of learning-based methods. In this work, we propose ‘Dexterous Imitation Made Easy’ (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera to observe a human operator and teleoperate our robotic hand. Once demonstrations are collected, DIME employs standard imitation learning methods to train dexterous manipulation policies. On both simulation and real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as ‘flipping’, ‘spinning’, and ‘rotating’ objects with the Allegro hand. Our framework along with pre-collected demonstrations is publicly available at the project URL.&lt;/p&gt;</content><author><name></name></author><category term="research" /><category term="robotics," /><category term="nearest" /><category term="neighbors," /><category term="representation" /><category term="learning," /><category term="imitation" /><category term="learning," /><category term="in-hand" /><category term="manipulation" /><summary type="html">Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement learning having been previously explored in literature. Perhaps one of the most powerful techniques to learn complex manipulation strategies is imitation learning. However, collecting and learning from demonstrations in dexterous manipulation is quite challenging. The complex, high-dimensional action-space involved with multi-finger control often leads to poor sample efficiency of learning-based methods. In this work, we propose ‘Dexterous Imitation Made Easy’ (DIME) a new imitation learning framework for dexterous manipulation. DIME only requires a single RGB camera to observe a human operator and teleoperate our robotic hand. Once demonstrations are collected, DIME employs standard imitation learning methods to train dexterous manipulation policies. On both simulation and real robot benchmarks we demonstrate that DIME can be used to solve complex, in-hand manipulation tasks such as ‘flipping’, ‘spinning’, and ‘rotating’ objects with the Allegro hand. Our framework along with pre-collected demonstrations is publicly available at the project URL.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/dime.gif" /><media:content medium="image" url="http://localhost:4000/assets/dime.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Holo-Dex - Teaching Dexterity with Immersive Mixed Reality</title><link href="http://localhost:4000/2022/09/28/holodex/" rel="alternate" type="text/html" title="Holo-Dex - Teaching Dexterity with Immersive Mixed Reality" /><published>2022-09-28T19:39:20-04:00</published><updated>2022-09-28T19:39:20-04:00</updated><id>http://localhost:4000/2022/09/28/holodex</id><content type="html" xml:base="http://localhost:4000/2022/09/28/holodex/">&lt;p&gt;A fundamental challenge in teaching robots is to provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated in dexterous manipulation, where teaching high-dimensional, contact-rich behaviors often require esoteric teleoperation tools. In this work, we present HOLO-DEX, a framework for dexterous manipulation that places a teacher in an immersive mixed reality through commodity VR headsets. The high-fidelity hand pose estimator onboard the headset is used to teleoperate the robot and collect demonstrations for a variety of general purpose dexterous tasks. Given these demonstrations, we use powerful feature learning combined with non-parametric imitation to train dexterous skills. Our experiments on six common dexterous tasks, including in-hand rotation, spinning, and bottle opening, indicate that HOLO-DEX can both collect high-quality demonstration data and train skills in a matter of hours. Finally, we find that our trained skills can exhibit generalization on objects not seen in training. Videos of HOLO-DEX are available in the project URL.&lt;/p&gt;</content><author><name></name></author><category term="research" /><category term="robotics," /><category term="in-hand" /><category term="manipulation," /><category term="virtual" /><category term="reality," /><category term="nearest" /><category term="neighbors," /><category term="representation" /><category term="learning," /><category term="imitation" /><category term="learning" /><summary type="html">A fundamental challenge in teaching robots is to provide an effective interface for human teachers to demonstrate useful skills to a robot. This challenge is exacerbated in dexterous manipulation, where teaching high-dimensional, contact-rich behaviors often require esoteric teleoperation tools. In this work, we present HOLO-DEX, a framework for dexterous manipulation that places a teacher in an immersive mixed reality through commodity VR headsets. The high-fidelity hand pose estimator onboard the headset is used to teleoperate the robot and collect demonstrations for a variety of general purpose dexterous tasks. Given these demonstrations, we use powerful feature learning combined with non-parametric imitation to train dexterous skills. Our experiments on six common dexterous tasks, including in-hand rotation, spinning, and bottle opening, indicate that HOLO-DEX can both collect high-quality demonstration data and train skills in a matter of hours. Finally, we find that our trained skills can exhibit generalization on objects not seen in training. Videos of HOLO-DEX are available in the project URL.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/holodex.gif" /><media:content medium="image" url="http://localhost:4000/assets/holodex.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Surprising Effectiveness of Representation Learning for Visual Imitation</title><link href="http://localhost:4000/2022/09/28/vinn/" rel="alternate" type="text/html" title="The Surprising Effectiveness of Representation Learning for Visual Imitation" /><published>2022-09-28T19:37:20-04:00</published><updated>2022-09-28T19:37:20-04:00</updated><id>http://localhost:4000/2022/09/28/vinn</id><content type="html" xml:base="http://localhost:4000/2022/09/28/vinn/">&lt;p&gt;While visual imitation learning offers one of the most effective ways of learning from visual demonstrations, generalizing from them requires either hundreds of diverse demonstrations, task specific priors, or large, hard-to-train parametric models. One reason such complexities arise is because standard visual imitation frameworks try to solve two coupled problems at once: learning a succinct but good representation from the diverse visual data, while simultaneously learning to associate the demonstrated actions with such representations. Such joint learning causes an interdependence between these two problems, which often results in needing large amounts of demonstrations for learning. To address this challenge, we instead propose to decouple representation learning from behavior learning for visual imitation. First, we learn a visual representation encoder from offline data using standard supervised and self-supervised learning methods. Once the representations are trained, we use non-parametric Locally Weighted Regression to predict the actions. We experimentally show that this simple decoupling improves the performance of visual imitation models on both offline demonstration datasets and real-robot door opening compared to prior work in visual imitation. All of our generated data, code, and robot videos are publicly available at the project URL.&lt;/p&gt;</content><author><name></name></author><category term="research" /><category term="robotics," /><category term="nearest" /><category term="neighbors," /><category term="representation" /><category term="learning," /><category term="imitation" /><category term="learning" /><summary type="html">While visual imitation learning offers one of the most effective ways of learning from visual demonstrations, generalizing from them requires either hundreds of diverse demonstrations, task specific priors, or large, hard-to-train parametric models. One reason such complexities arise is because standard visual imitation frameworks try to solve two coupled problems at once: learning a succinct but good representation from the diverse visual data, while simultaneously learning to associate the demonstrated actions with such representations. Such joint learning causes an interdependence between these two problems, which often results in needing large amounts of demonstrations for learning. To address this challenge, we instead propose to decouple representation learning from behavior learning for visual imitation. First, we learn a visual representation encoder from offline data using standard supervised and self-supervised learning methods. Once the representations are trained, we use non-parametric Locally Weighted Regression to predict the actions. We experimentally show that this simple decoupling improves the performance of visual imitation models on both offline demonstration datasets and real-robot door opening compared to prior work in visual imitation. All of our generated data, code, and robot videos are publicly available at the project URL.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/vinn.gif" /><media:content medium="image" url="http://localhost:4000/assets/vinn.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>